---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Name and EID here Chengmin Xu, cx2546

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

Introduction:
Graduation is approaching, one of the most important things come to my mind is job. Getting a good job and earning good salary are dreams for everyone. But, what factors actually affect wage? Does education really matter? What about IQ and other factors?What would predictions look like based on the current data? These questions rouse my interest in exploring the relationship between wage and these factors by using data software(e.g. R-Studio and Python) to analyze.  

The data set we use in this project is the publicly available data from a research of my major.It is a survey over 200 adults and contains 13 variables.The main variables include "wage"( monthly earnings), "hour"(average weekly hours),"IQ"(IQ score), "exper"(years of work experience),"age"(age in years), "married"(=1 if married), "black"(=1 if black), "urtban"(=1 if live in SMSA)etc.There are 24 observations for unmarried people, and 176 observations for married people;165 observations for non-black people, and 35 observations for black people.

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
data_wage <- read_csv("new.csv")

```

### Cluster Analysis

```{R}
library(cluster)
# clustering code here
wage_pam <- data_wage%>%pam(k=3)
pamclust<-data_wage %>% mutate(cluster = as.factor(wage_pam$clustering))
pamclust%>%ggplot(aes(KWW, wage, shape=cluster, color=black))+geom_point()

pamclust<-data_wage %>% mutate(cluster = as.factor(wage_pam$clustering))
pamclust%>%ggplot(aes(educ, wage, shape=cluster, color=urban))+geom_point()

pamclust<-data_wage %>% mutate(cluster = as.factor(wage_pam$clustering))
pamclust%>%ggplot(aes(IQ, wage, shape=cluster, color=married))+geom_point()

sil_width <- vector()
for (i in 2:10) {
    pam_fit <- pam(data_wage, k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) + 
    scale_x_continuous(name = "k", breaks = 1:10)

wage_pam <- data_wage%>%pam(k=3)
plot(wage_pam)

data_wage%>% slice(wage_pam$id.med)


library(GGally)
data_wage %>% mutate(cluster = as.factor(wage_pam$clustering)) %>% 
    ggpairs(columns =2:9, aes(color = cluster))

```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
data_wage2<-data_wage %>% select(-1,-10,-11,-12,-13)
pca_data_wage2 <- princomp(data_wage2, cor = T)
names(pca_data_wage2)
summary(pca_data_wage2, loadings=T)

eigval <-pca_data_wage2$sdev^2
varprop=round(eigval/sum(eigval), 2)
  
ggplot() + geom_bar(aes(y=varprop, x=1:8), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:8))+
geom_text(aes(x=1:8, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5)+
scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) +
scale_x_continuous(breaks=1:10)

round(cumsum(eigval)/sum(eigval), 2)

data_wage2 %>% mutate(PC1=pca_data_wage2$scores[, 1], PC2=pca_data_wage2$scores[, 2]) %>%
ggplot(aes(PC1, PC2, color=data_wage$married)) + geom_point() + coord_fixed()

library(factoextra)
fviz_pca_biplot(pca_data_wage2)
fviz_pca_biplot(pca_data_wage2, col.ind = data_wage$married, ) + coord_fixed()
fviz_pca_biplot(pca_data_wage2, col.ind = data_wage$black, ) + coord_fixed()
fviz_pca_biplot(pca_data_wage2, col.ind = data_wage$urban, ) + coord_fixed()

#gower dissimilarities
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
# linear classifier code here
#logistic regression
class_dat <- data_wage %>% select(married, wage:age)
glimpse(class_dat) 
fit<-glm(married ~ . , data=class_dat, family="binomial") 
probs <- predict(fit, type="response")
probs
class_diag(probs, truth =class_dat$married, positive=1)
y_hat <- ifelse(probs>0.5,"1","0")
y_hat <- factor(y_hat, levels=c("0","1"))
table(truth = class_dat$married, predictions=y_hat)
#table

```

```{R}
# cross-validation of linear classifier here
#K-fold CV
k=10 
data<-sample_frac(class_dat)
folds<-rep(1:k,length.out=nrow(data))

diags<-NULL

i=1

for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$married

fit<-glm(married~.,data=train,family="binomial")
probs<-predict(fit,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)


```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
class_dat <- data_wage %>% select(married, wage:age)
knn_fit <- knn3(married ~ ., data = class_dat)
probs_knn <- predict(knn_fit, class_dat)[, 2]
probs_knn
class_diag(probs_knn, class_dat$married, positive = 1)
y_hat_knn <- ifelse(probs_knn>0.5,"1","0")
y_hat_knn <- factor(y_hat_knn, levels=c("0","1"))
table(truth = class_dat$married, predictions=y_hat_knn)

```

```{R}
# cross-validation of np classifier here
library(caret)
k=10 
data<-sample_frac(class_dat)
folds<-rep(1:k,length.out=nrow(data))

diags<-NULL

i=1

for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$married

knn_fit<-knn3(married~.,data=train)
probs_knn<-predict(knn_fit,newdata = test)[,2]

diags<-rbind(diags,class_diag(probs_knn,truth, positive=1))
}
summarize_all(diags,mean)
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here

fit_linear <- lm(IQ ~educ+KWW+exper,data=data_wage)
props_linear <- predict(fit_linear)
props_linear

fit_linear_sum <-summary(fit_linear)
mean(fit_linear_sum$residuals^2)

```

```{R}
# cross-validation of regression model here
k=10 
data<-sample_frac(data_wage)
folds<-rep(1:k,length.out=nrow(data))

MSE<-NULL

i=1

for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]

fit_linear <- lm(IQ ~educ+KWW+exper,data=train)
probs_linear<-predict(fit_linear ,newdata = test,type="response")
MSE<-rbind(MSE,mean(summary(fit_linear)$residuals^2))
}

mean(MSE)

```

Discussion

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)
wage_median<-median(data_wage$wage)
wage_mean<-mean(data_wage$wage)
```

```{python}
# python code here
next1="The median wage of our data set is"
next2="The average wage of our data set is"
print(next1, r.wage_median)
```

```{R}
use_python("/usr/bin/python3", required = F)
cat(c(py$next2 ,wage_mean))

```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




